{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saleen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = pd.read_csv('spam.csv',encoding='Windows-1252')\n",
    "# Dropping the redundent looking collumns (for this project)\n",
    "to_drop = [\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"]\n",
    "sms = sms.drop(sms[to_drop], axis=1)\n",
    "# Renaming the columns because I feel fancy today \n",
    "sms.rename(columns = {\"v1\":\"label\", \"v2\":\"message\"}, inplace = True)\n",
    "# ham_rows = sms[sms['label'] == 'ham']\n",
    "# num_rows_to_delete = int(0.54 * len(ham_rows))\n",
    "# rows_to_delete = np.random.choice(ham_rows.index, num_rows_to_delete, replace=False)\n",
    "# sms.drop(rows_to_delete, inplace=True)\n",
    "sms.head()\n",
    "corpus = []\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n"
     ]
    }
   ],
   "source": [
    "sms.head()\n",
    "print(sms.shape[0])\n",
    "for i in range(0,sms.shape[0]):\n",
    "    message = re.sub(pattern=r'[^a-zA-Z]', repl=' ', string=str(sms.message[i])) #Cleaning special character from the message\n",
    "    # print(message)\n",
    "    message = message.lower() #Converting the entire message into lower case\n",
    "    # print(message)\n",
    "    words = message.split() # Tokenizing the review by words\n",
    "    # print(words)\n",
    "    words = [word for word in words if word not in set(stopwords.words('english'))] #Removing the stop words\n",
    "    # print(words)\n",
    "    words = [ps.stem(word) for word in words] #Stemming the words\n",
    "    # print(words)\n",
    "    message = ' '.join(words) #Joining the stemmed words\n",
    "    # print(message)\n",
    "    corpus.append(message) #Building a corpus of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cv = CountVectorizer(max_features=2500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(sms[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Accuracy Score 98.65 %\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(y_pred)\n",
    "acc_s = accuracy_score(y_test, y_pred)*100\n",
    "print(\"Accuracy Score {} %\".format(round(acc_s,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and the preprocessed data to disk\n",
    "with open(r'sms\\archive\\pickle\\sms_model2.pkl', 'wb') as f:\n",
    "    pickle.dump(classifier, f)\n",
    "with open(r'sms\\archive\\pickle\\preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump((cv, X, y), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
